{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audiobooks business case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Machine Learning problem which consists on estimating the likelihood of an audiobook customer making another purchase (conversion), given past purchases and usage information. If a potential recurring customer can be identified, marketing campaings can be more effectively by focusing efforts and resources to these clients.  \n",
    "\n",
    "We were given a raw .csv file  which needs to processed in order to extrac relevant data to build the model. Our file represents 2 years of past user engagement data and a final column indicating if any purchase was made by the customer in the following 6 months after that period. In other words, past data contains the inputs and purchase info is the target of the model.\n",
    "\n",
    "The present notebook is used to preprocess the data and generate a suitable file format (.npz) with useful information to further modeling. The actual modeling is split in another notebook to avoid recreating the file when the kernel is restarted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the data from the csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has the following columns:\n",
    "\n",
    "\n",
    "[ID, Overall Book Length (mins), Average Book Length (mins), Overall Price, Average Price, Review, Review 10/10, Minutes Listened, Completition, Support Requests, Last visited minus purchase data, Targets]\n",
    "\n",
    "\n",
    "\n",
    "The user ID does not provide any relevant information and will be dropped. The final 'Targets' columns indicates if the user converted in the following 6 months. The remaining are the inputs which will be used to predict purchases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:08:55.683745Z",
     "start_time": "2020-12-09T17:08:54.019246Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "data_raw = np.loadtxt ('Audiobooks_data.csv', delimiter=',')\n",
    "inputs_raw = data_raw[:,1:-1]\n",
    "targets_raw = data_raw[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "First, the data needs to be shuffled thus avoiding our model to become biased if there is some kind of data sorting in the file, such as day of the week, time of purchase and so.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:08:55.696749Z",
     "start_time": "2020-12-09T17:08:55.687743Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffling the indices\n",
    "shuffled_indices = np.arange(inputs_raw.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Reassing the indices to shuffled indices\n",
    "inputs_shuffled = inputs_raw[shuffled_indices]\n",
    "targets_shuffled = targets_raw[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balance the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to make sure the data is balanced, since the number of users which converted is probaly not equal to those who didn't. An imbalance could result in a model that performs poorly in real life, since the training data was biased towards a certain result. In this audiobook problem, 'did not convert' represents 84% of the data, so the model would most likely assing this result when subjected to real world data. In that way, we choose to reject some data to obtain a model with better predictbility. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:08:55.729811Z",
     "start_time": "2020-12-09T17:08:55.700094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Since the data is shuffled, we'll randomly remove indices until the number of users that converted is equal to those who didn't\n",
    "# Considering purchases are represented by 1, we can sum all values to count the total number\n",
    "n_target_one = targets_shuffled.sum()\n",
    "\n",
    "counter_target_zero = 0\n",
    "indices_to_remove = []\n",
    "\n",
    "for i in range (targets_shuffled.shape[0]):\n",
    "    if targets_shuffled[i] == 0:\n",
    "        counter_target_zero += 1\n",
    "        if counter_target_zero > n_target_one:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "inputs_prior = np.delete(inputs_shuffled, indices_to_remove, axis=0)\n",
    "targets_prior = np.delete(targets_shuffled, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-shuffle the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the indices were not reset after the balancing step, it's necessary to re-shuffle the data. Otherwise, most of the target 1s would be located by the end of the dataset, since some zeroes were removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:08:55.739768Z",
     "start_time": "2020-12-09T17:08:55.732773Z"
    }
   },
   "outputs": [],
   "source": [
    "indices_prior = np.arange(inputs_prior.shape[0])\n",
    "np.random.shuffle(indices_prior)\n",
    "\n",
    "inputs_prior = inputs_prior[indices_prior]\n",
    "targets_prior = targets_prior[indices_prior]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good practice is to scale the imputs, which improves the learning process, since they initialy have very different orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:08:55.752769Z",
     "start_time": "2020-12-09T17:08:55.743766Z"
    }
   },
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(inputs_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll split the data and use 80% of it to train the model, 10% to validate and the final 10% to perform a final test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:08:55.766766Z",
     "start_time": "2020-12-09T17:08:55.754765Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.502374965074043\n",
      "0.49217002237136465\n",
      "0.4888392857142857\n"
     ]
    }
   ],
   "source": [
    "num_samples = inputs_prior.shape[0]\n",
    "\n",
    "train_samples = int(0.8 * num_samples)\n",
    "valid_samples = int(0.1 * num_samples)\n",
    "test_samples = num_samples - train_samples - valid_samples\n",
    "\n",
    "train_inputs = scaled_inputs[:train_samples]\n",
    "train_targets = targets_prior[:train_samples]\n",
    "\n",
    "valid_inputs = scaled_inputs[train_samples : train_samples + valid_samples]\n",
    "valid_targets = targets_prior[train_samples : train_samples + valid_samples]\n",
    "\n",
    "test_inputs = scaled_inputs[train_samples + valid_samples : train_samples + valid_samples + test_samples]\n",
    "test_targets = targets_prior[train_samples + valid_samples : train_samples + valid_samples + test_samples]\n",
    "\n",
    "# Checking the balance. They should be roughly 0.5\n",
    "print(train_targets.sum()/train_samples)\n",
    "print(valid_targets.sum()/valid_samples)\n",
    "print(test_targets.sum()/test_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the three datasets in .npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:09:59.402946Z",
     "start_time": "2020-12-09T17:09:59.388947Z"
    }
   },
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_valid', inputs=valid_inputs, targets=valid_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
