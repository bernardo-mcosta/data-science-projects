{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfIPztQZzu8B"
   },
   "source": [
    "# Deep Neural Network for MNIST Classification\n",
    "\n",
    "We'll apply all the knowledge from the lectures in this section to write a deep neural network. The problem we've chosen is referred to as the \"Hello World\" of deep learning because for most students it is the first deep learning algorithm they see.\n",
    "\n",
    "The dataset is called MNIST and refers to handwritten digit recognition. You can find more about it on Yann LeCun's website (Director of AI Research, Facebook). He is one of the pioneers of what we've been talking about and of more complex approaches that are widely used today, such as covolutional neural networks (CNNs). \n",
    "\n",
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image). \n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only 10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes. \n",
    "\n",
    "Our goal would be to build a neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8PdgJSUzu8K"
   },
   "source": [
    "## Import the relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:58:55.092012Z",
     "start_time": "2020-12-09T17:58:50.606502Z"
    },
    "id": "1zUsKi_2zu8L"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "# tensorflow  will be used to load the ready-to-use MNIST dataset\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4B9UBbkvzu8M"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:58:58.179749Z",
     "start_time": "2020-12-09T17:58:55.098015Z"
    },
    "id": "vF-Pu_JHzu8M"
   },
   "outputs": [],
   "source": [
    "# Using 'tfds.load' method to import MNIST dataset\n",
    "# Using 'with_info=True' to obtain relevant information regarding number of samples\n",
    "# Using 'as_supervised=True' to split the dataset in a 2-tuple structure (input, target) \n",
    "mnist_data, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
    "\n",
    "# The 'mnist_dataset' variable already contains a train and testing split, which will be extracted\n",
    "mnist_train, mnist_test = mnist_data['train'], mnist_data['test']\n",
    "\n",
    "# Obtaining the number of training samples from the 'mnist_info' metadata\n",
    "# Defining the number of the validation set samples as a % of the train set\n",
    "# Defining the number of the test set samples\n",
    "num_valid = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_test = mnist_info.splits['test'].num_examples\n",
    "\n",
    "# The above step produces a float, which now will be turn into a int to avoid issues\n",
    "num_valid = tf.cast(num_valid, tf.int64)\n",
    "num_test = tf.cast(num_test, tf.int64)\n",
    "\n",
    "# Preparing a data scaler to obtain inputs between 0 and 1\n",
    "# The inputs are shades of gray numered from 0 to 255\n",
    "def scale (image,label): \n",
    "    image = tf.cast(image, tf.float64)\n",
    "    image /= 255.\n",
    "    return image, label\n",
    "\n",
    "# Scaling the data set applying the scaler with '.map()'\n",
    "# The 'mnist_train' contains both train and validation datasets, which later will be split using 'num_valid'\n",
    "scaled_train_valid_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)\n",
    "\n",
    "# Shuffling the data to ensure randomness during training\n",
    "# There's no need to shuffle the test data\n",
    "buffer_size = 10000\n",
    "shuffled_train_valid_data = scaled_train_valid_data.shuffle(buffer_size)\n",
    "\n",
    "# Finnaly splitting train and validation data sets, after ensuring they're shuffled\n",
    "# Validation takes only the first 'num_valid' samples. Train skips the first 'num_valid' samples.\n",
    "validation_data = shuffled_train_valid_data.take(num_valid)\n",
    "train_data = shuffled_train_valid_data.skip(num_valid)\n",
    "\n",
    "# Batch size\n",
    "batch_size = 100\n",
    "\n",
    "# Combining data into batches, as required by the model\n",
    "# Testing and validation are a single batch, since they do not back propagate (weight adjustments)\n",
    "train_data = train_data.batch(batch_size)\n",
    "validation_data = validation_data.batch(num_valid)\n",
    "test_data = test_data.batch(num_test)\n",
    "\n",
    "# takes next batch (it is the only batch)\n",
    "# because as_supervized=True, we've got a 2-tuple structure\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n16etv0szu8N"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wB56D4jMzu8N"
   },
   "source": [
    "### Outline the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:58:58.270765Z",
     "start_time": "2020-12-09T17:58:58.181751Z"
    },
    "id": "AvlIgNzezu8O"
   },
   "outputs": [],
   "source": [
    "# inputs are pixels in a 28 x 28 x 1 image\n",
    "# outputs are digits ranging from 0 to 9\n",
    "input_size = 28 * 28 * 1\n",
    "output_size = 10\n",
    "\n",
    "# Defining hidden layer size\n",
    "hidden_layer_size = 600\n",
    "\n",
    "# The model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)), #input layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), #1 Hidden Layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), #2 Hidden Layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), #3 Hidden Layer\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') #output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4qxuV91zu8O"
   },
   "source": [
    "### Otimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T17:58:58.408767Z",
     "start_time": "2020-12-09T17:58:58.274754Z"
    },
    "id": "9bbBFsUKzu8P"
   },
   "outputs": [],
   "source": [
    "# Using 'adam' stochastic descent method for weight adjustment (optimizer)\n",
    "# Since it's a classification problem and the labels are not in one-hot format, the loss function is sparse cross-entropy\n",
    "model.compile(optimizer= 'adam', loss= 'sparse_categorical_crossentropy', metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zQsLxwlHzu8P"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T18:01:02.060508Z",
     "start_time": "2020-12-09T17:58:58.410754Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90085,
     "status": "ok",
     "timestamp": 1607707254172,
     "user": {
      "displayName": "Bernardo Costa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhIUSup0zeTXEySOZPkAaw0K6Yhwwl7FfFrOzTC_bQ=s64",
      "userId": "08543559093626801139"
     },
     "user_tz": 180
    },
    "id": "gc1UW4Fozu8P",
    "outputId": "48aa1cfa-65bd-4989-8cf5-bebaf362bb68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "540/540 - 13s - loss: 0.2132 - accuracy: 0.9342 - val_loss: 0.1036 - val_accuracy: 0.9677\n",
      "Epoch 2/8\n",
      "540/540 - 10s - loss: 0.0841 - accuracy: 0.9740 - val_loss: 0.0979 - val_accuracy: 0.9710\n",
      "Epoch 3/8\n",
      "540/540 - 10s - loss: 0.0607 - accuracy: 0.9804 - val_loss: 0.0569 - val_accuracy: 0.9812\n",
      "Epoch 4/8\n",
      "540/540 - 9s - loss: 0.0443 - accuracy: 0.9861 - val_loss: 0.0676 - val_accuracy: 0.9800\n",
      "Epoch 5/8\n",
      "540/540 - 10s - loss: 0.0377 - accuracy: 0.9878 - val_loss: 0.0560 - val_accuracy: 0.9827\n",
      "Epoch 6/8\n",
      "540/540 - 10s - loss: 0.0343 - accuracy: 0.9892 - val_loss: 0.0339 - val_accuracy: 0.9887\n",
      "Epoch 7/8\n",
      "540/540 - 10s - loss: 0.0260 - accuracy: 0.9921 - val_loss: 0.0310 - val_accuracy: 0.9902\n",
      "Epoch 8/8\n",
      "540/540 - 9s - loss: 0.0242 - accuracy: 0.9925 - val_loss: 0.0268 - val_accuracy: 0.9908\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f83fee54278>"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 8\n",
    "\n",
    "model.fit(train_data, epochs = num_epochs, validation_data=(validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_V0EMvfvzu8R"
   },
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T18:01:03.831507Z",
     "start_time": "2020-12-09T18:01:02.063509Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 91593,
     "status": "ok",
     "timestamp": 1607707255695,
     "user": {
      "displayName": "Bernardo Costa",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhIUSup0zeTXEySOZPkAaw0K6Yhwwl7FfFrOzTC_bQ=s64",
      "userId": "08543559093626801139"
     },
     "user_tz": 180
    },
    "id": "Rye3auWkzu8R",
    "outputId": "96267a50-9583-42c6-aa97-6d1a6e6471b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.9815\n",
      "\n",
      "Test loss: 0.08. Test accuracy: 98.15%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1e_FVSTzu8S"
   },
   "source": [
    "## Takeaway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9n6BtCGGzu8S"
   },
   "source": [
    "The model achieved an accuracy of roughly 98.0% when predicting images and assignin a digit to it. It's a good result, considering the methodology used and the time it took to train itself in a matter of just a couple minutes. If we were to improve the accuracy even by a small increase such as 0.5%, it could take a large amount of processing time, maybe even a few hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fp8sSJl50w2L"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "TensorFlow_MNIST_All_Exercises-Copy1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
